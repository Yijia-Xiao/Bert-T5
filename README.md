# Bert and T5 protein language models

`Experiments of protein language models on fluorescence prediction. Regression task: [input] protein sequence, [output] fluorescence (real-valued label)`


## Preprocess
- Embed the protein sequences using pre-trained language models (Bert, T5)


## Regression
- No joint fine-tuning is applied
    - The parameters of pre-trained language models are *freezed* in the training stage


## To-Do
- Experiment with MSA-based methods
    - Take the representation of query sequence to do downstream tasks
    - Possible problem: the dataset is generated by applying mutations to GFP (green fluorescence protein), so there might be only a few homologs of the protein.
- Add fine-tuning codes